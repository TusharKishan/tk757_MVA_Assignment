---
title: "Untitled"
author: "Tushar"
date: "2024-04-27"
output: html_document
---

```{r}
library(ggplot2)
library(memisc)
library(ROCR)
library(dplyr)
library(klaR)

Laptop <- read.csv("/Users/tusharkishan/Desktop/Multivariate/Assignment\ 1/Laptop_price.csv")
str(Laptop)

r1 <- lda(formula = Brand ~ ., data = Laptop)
r1
#Ans) The output predicts aptop brands (Acer, Asus, Dell, HP, Lenovo) based on several features (Processor_Speed, RAM_Size, Storage_Capacity, Screen_Size, Weight, Price).Prior probabilities: These are the estimated probabilities of each brand occurring in the dataset.
#Group means: These are the mean values of each feature for each brand.
#Coefficients of linear discriminants: These coefficients are used to linearly combine the features to form the discriminant functions (LD1, LD2, LD3, LD4).
#Proportion of trace: This indicates the proportion of the total variance explained by each discriminant function. The first discriminant function (LD1) explains 52.91% of the variance, the second (LD2) explains 30.94%, the third (LD3) explains 15.46%, and the fourth (LD4) explains 0.70%.

r2 <- lda(formula = Brand ~ ., data = Laptop, CV=TRUE)
r2
#Ans) The class column shows the predicted class for each observation, and the posterior columns show the posterior probabilities for each class. Each row corresponds to an observation, and each column corresponds to a class.These probabilities represent the model's confidence in each class prediction for that observation.

train <- sample(1:500, 95)
r3 <- lda(Brand ~ ., data = Laptop, prior = c(0.2, 0.2, 0.2, 0.2, 0.2), subset = train)
r3
#Ans) Here's the summary of my findings
#Prior Probabilities of Groups: These are the prior probabilities for each class (brand) in your dataset. In this case, you've set them to be equal (0.2 for each brand).
#Group Means: These are the mean values of the predictor variables (Processor_Speed, RAM_Size, Storage_Capacity, Screen_Size, Weight, Price) for each brand. They provide insight into how the brands differ in terms of these features.
#Coefficients of Linear Discriminants: These coefficients are used to construct linear combinations of the predictor variables that best separate the classes. They indicate the importance of each predictor in the classification.
#Proportion of Trace: This shows the proportion of total variance in the data explained by each linear discriminant. In this case, LD1 explains 58.6% of the variance, LD2 explains 30.9%, LD3 explains 7.9%, and LD4 explains 2.5%.

llda1 = predict(object = r3, newdata = Laptop[-train, ])
head(llda1$class)

head(llda1$posterior, 50)

#Ans) Here's a brief explanation of the output
#llda1$class: This shows the predicted class for each observation in the test set. For example, the first observation is predicted to belong to the "Dell" class, the second to "Asus," and so on.
#llda1$posterior: This shows the posterior probabilities for each class for each observation. For example, for the first observation, the model assigns a high probability to the "Dell" class (0.73), indicating high confidence in this prediction.

head(llda1$x, 3)
#Ans) The llda1$x output shows the values of the linear discriminants (LD) for each observation in the test set. These values are essentially the coordinates of each observation in the LD space, which is a lower-dimensional space created by the LDA model to separate the classes.In my output each row represents an observation, and each column represents a different LD. For example, the first row shows the LD values for the first observation, where LD1 is -1.272716, LD2 is -1.6090542, LD3 is -1.4632172, and LD4 is 1.7946536. These LD values can be used to visualize the separation of classes in the LD space or for further analysis of the data.

plot(r1)
#Ans) We observe that there is a lot of overlap between the residuals

plot(r3)
#Ans) The overlap is way lesser compared to the previous plot

sample1 <- sample(c(TRUE, FALSE), nrow(Laptop), replace = T, prob = c(0.75,0.25))
train1 <- Laptop[sample1, ]
test1 <- Laptop[!sample1, ]

Laptop <- mean(Laptop, na.rm = TRUE)
lda.Laptop <- lda(Brand ~ ., train1)


plot(lda.Laptop, col = as.integer(train1$Brand))
#Ans) There is a lot of overlap between the residuals

lda.train1 <- predict(lda.Laptop)
train1$lda <- lda.train1$class
table(train1$lda,train1$Brand)
#Ans) The accuracy of the model is not great on the training data

lda.test1 <- predict(lda.Laptop,test1)
test1$lda <- lda.test1$class
table(test1$lda,test1$Brand)

#Ans) The accuracy of the model of the test data is better than the training data

```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
